{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0516ba-5424-4b6d-9328-108ed9c39a77",
   "metadata": {},
   "source": [
    "**Using Brev.dev's single A10G with 24GB GPU Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef23332-738c-4163-9cea-b79c0f15f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run this on each device once\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f8b00-a976-461d-9431-0071a72df8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixes any updating issues\n",
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb3199-8f4f-4269-95e1-a7e8edfc38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='notes.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='notes_validation.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a42d3-dd0f-487e-9f54-020ef3753408",
   "metadata": {},
   "source": [
    "**Training Metrics**: using Weights & Biases to keep track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3bf94-472c-47fa-a052-3fdcebad2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to get data report on Weights & Biases \n",
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"journal-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cb58e-eaf7-4aa8-b367-8f3ce14086e8",
   "metadata": {},
   "source": [
    "**Formatting Function**: structures training examples into prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656320c4-e717-4961-8fe3-fe6fd6396af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2e950-1a36-49c9-9fe6-bbaa0bdec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Loading Mistral** - mistralai/Mistral-7B-v0.1 - using 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb17b2-4da5-439e-a4bb-08a07cffdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04084042-a493-4904-9a48-416b3243c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53a808-be11-454f-8d4e-ae8b9693be1c",
   "metadata": {},
   "source": [
    "**Tokenization**: setting up tokenizer and adding left padding (training uses less memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ac76f-b1fc-485e-805f-038811211648",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3eaa3-355f-4078-9520-1cba641311ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59500969-fc2b-4372-a31e-b20c0d3d4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to plot data lengths \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b48b42-1687-45b9-a463-5fd345ddb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Padding**: adding padding so that all prompts are same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ee329-d610-4454-b027-55c916d00c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 105 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b5b21-d21b-4308-9367-c54381df56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b67d0-c9fa-4e77-b2fa-7f858a18d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure data has correct padding\n",
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65be27-e3ec-4337-8963-c8ffb45d20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting data lengths to make sure that all the data is same length (105)\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ed72d-8796-4ee2-95a8-c0b46335cd4c",
   "metadata": {},
   "source": [
    "**The following prompts are used to test the base model to compare it later to the trained model. You can replace the eval_prompt with any questions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a5bc8-d5ae-498b-babc-8b66bbacb8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290e0af-6bb2-46c0-b956-bde5cff72f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \" What are the differences between Gramsci's and Plato's beliefs about reality? \"\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4ad26-0dd7-4495-848d-e309977b08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \" What are problems with utilitarianism? \"\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322fcd3-8bbf-4425-9cd9-ba549d8eee7e",
   "metadata": {},
   "source": [
    "**Preprocessing**: done before setting up LoRA to prepare it for finetuning, done using kit from PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d1852-a848-455c-a24b-000f7b34f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2ee79-cf11-4454-8744-f45347252c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7534285-e519-4075-ac0c-c5ad3b4eda41",
   "metadata": {},
   "source": [
    "**Printing Model**: the linear layers that QLoRA will be applied to (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj, and lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d33d1-242b-4c11-b53a-1d5dc4ef00bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9de19-a6e5-4a02-9dc8-fc8e1fae1e1a",
   "metadata": {},
   "source": [
    "**LoRA Config**: \n",
    "\n",
    "`r`: rank of the low-rank matrix used in the adapters, which controls number of parameters trained\n",
    "\n",
    "`alpha`: scaling factor for the learned weights; the weight matrix is scaled by `alpha/r`\n",
    "\n",
    "Using `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d28087-0ae9-4a5d-8f16-3e845ee96186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542c7d4-8048-4974-9cc5-79536993c005",
   "metadata": {},
   "source": [
    "**See New LoRA adaptors!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93618da1-2410-42b7-b152-18b73b5b70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1cf453-683a-465a-a0b4-ed30e7b00f26",
   "metadata": {},
   "source": [
    "**Running Training**: used 'max_steps' = 500, because I did not mind overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187edf25-8c68-47a7-b4cd-b90c077908f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb27c4-ea79-428d-a9ef-836b59412d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66462b16-0899-448b-917e-36aa6a5a2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=2,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5, \n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              \n",
    "        logging_dir=\"./logs\",        \n",
    "        save_strategy=\"steps\",       \n",
    "        save_steps=25,                \n",
    "        evaluation_strategy=\"steps\", \n",
    "        eval_steps=25,               \n",
    "        do_eval=True,                \n",
    "        report_to=\"wandb\",           \n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          \n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0693152-2ac6-4019-b280-4ac5971b37d5",
   "metadata": {},
   "source": [
    "**Trained Model**: used checkpoint-125; need to reload base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e298b-be1f-4eb7-a1fa-c36851d027ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d37b0-46e5-4a16-ac42-9f25c6fc7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-journal-finetune/checkpoint-125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74110c5-9534-4655-b3be-b8c7d994945d",
   "metadata": {},
   "source": [
    "**Fine-tuned Model**: testing new model on questions used earlier on base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c99af6-9d25-4947-b022-e3e70fe6bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \" How does direction of fit apply to social media? \"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992c592-3fbf-408b-a86a-fe1b97020de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \" What are the differences between Gramsci's and Plato's beliefs about reality? \"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f876065-b163-4f90-bf7e-5882e3e41efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \" What are problems with utilitarianism? \"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
